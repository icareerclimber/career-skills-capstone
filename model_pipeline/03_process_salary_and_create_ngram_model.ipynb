{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Naive Bayes classifier using TD-IDF vectorizer and pickle results. This model can be \n",
    "# trained on job resumes or Indeed job postings. Steps include:\n",
    "# 1. Set the model parameters\n",
    "# 2. Getting a combined list of salaries and only using job titles with 500+ salary records\n",
    "# 3. Getting a list of resumes using this list of job titles and remove any job titles with \n",
    "#    less than 500 resumes\n",
    "# 4. Run TD-IDF vectorizer and Naive Bayes model training\n",
    "# 5. Test the model using \"List Most Relevant Skills\"\n",
    "# 6. Test the model using \"Document Similarity Score\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from itertools import product\n",
    "from random import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "\n",
    "# Custom function in functions folder\n",
    "from functions.word_preprocessing import *\n",
    "\n",
    "pd.set_option('display.max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# directory = '/Users/kwheatley/Desktop/Capstone/gcloud_data/'\n",
    "directory = '/mnt/disks/mnt_dir/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "                \"doc_type\":\"indeed_resume\", # Use \"indeed_resume\" or \"indeed_postings\"\n",
    "                \"min_salary_records\":100, # Filter out all jobs with less than specified salary records\n",
    "                \"min_job_summaries\":1000, # Filter out all jobs with less than specified job summaries\n",
    "                \"min_ngram\":3, # For TD-IDF vectorizer\n",
    "                \"max_ngram\":4, # For TD-IDF vectorizer\n",
    "                \"min_df\":5, # For TD-IDF vectorizer, ignore features in less than this number of documents\n",
    "                \"train_test_split\":0.05, # For train-test split\n",
    "                \"random_state\":1, # For train-test split\n",
    "                \"alpha\":0.2, # For Naive Bayes model 0.02 is current best\n",
    "                \"num_skills\":50, # Number of skill to show per job \n",
    "                \"max_number_records\":10000, # Number of records in each class, SMOTE is used to fill small classes\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Salary Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the both salary datasets\n",
    "salary1 = pd.read_csv(directory+'02_salaries_h1b.csv')\n",
    "salary2 = pd.read_csv(directory+'02_salaries_greencard.csv')\n",
    "\n",
    "# Combine salary datasets\n",
    "temp_salary1 = salary1[['role','city','state','start_year','cleaned_job_title','experiences','salary']]\n",
    "temp_salary2 = salary2[['job_title','city','state','decision_year','cleaned_job_title','experiences','salary_amount']]\n",
    "temp_salary1.columns = ['original_role','city','state','start_year','cleaned_job_title','experiences','salary']\n",
    "temp_salary2.columns = ['original_role','city','state','start_year','cleaned_job_title','experiences','salary']\n",
    "combined_salaries = temp_salary1.append(temp_salary2)\n",
    "\n",
    "# Remove salaries with null value and convert to int\n",
    "combined_salaries = combined_salaries[~combined_salaries.salary.isnull()]\n",
    "combined_salaries.salary = combined_salaries.salary.astype(int)\n",
    "\n",
    "# Fill any NaN fields with no_value and convert each column into a list\n",
    "combined_salaries = combined_salaries.fillna('no_value')\n",
    "combined_salaries.experiences = combined_salaries.experiences.apply(lambda x: \n",
    "                                                list([item.strip() for item in x.split(',')]))\n",
    "combined_salaries.original_role = combined_salaries.original_role.apply(lambda x: [x])\n",
    "combined_salaries.city = combined_salaries.city.apply(lambda x: [x])\n",
    "combined_salaries.state = combined_salaries.state.apply(lambda x: [x])\n",
    "combined_salaries.start_year = combined_salaries.start_year.apply(lambda x: [x])\n",
    "combined_salaries.cleaned_job_title = combined_salaries.cleaned_job_title.apply(lambda x: [x])\n",
    "combined_salaries.salary = combined_salaries.salary.apply(lambda x: [x])\n",
    "\n",
    "# Perform a pivot on the columns to split out rows with multiple experience level qualifiers\n",
    "combined_salaries = pd.DataFrame([j for i in combined_salaries.values for j in product(*i)],\n",
    "                                      columns = combined_salaries.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get List of Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of jobs with 100+ salary records: 867\n"
     ]
    }
   ],
   "source": [
    "# Choose all jobs with `min_salary_records` or more records\n",
    "temp = combined_salaries.groupby('cleaned_job_title').count().salary.reset_index()\n",
    "jobs_to_model = temp[temp.salary >= parameters['min_salary_records']]\n",
    "pd.DataFrame(jobs_to_model.cleaned_job_title.unique()).to_csv(directory+'03_relevant_job_titles.csv',index=False)\n",
    "combined_salaries = combined_salaries[combined_salaries.cleaned_job_title\\\n",
    "                                                       .fillna('').isin(jobs_to_model.cleaned_job_title)]\n",
    "print(\"Number of jobs with \"+str(parameters['min_salary_records'])+\"+ salary records:\", jobs_to_model.cleaned_job_title.count())\n",
    "\n",
    "# combined_salaries.groupby(['cleaned_job_title'])\\\n",
    "# .salary.agg(['count','mean','min','max','median','std']).to_csv(\"sample.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Job Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load resume data\n",
    "if parameters['doc_type'] == 'indeed_resume':\n",
    "    data = pd.read_csv(directory+'02_resumes_work.csv')\n",
    "if parameters['doc_type'] == 'indeed_postings':\n",
    "    data = pd.read_csv(directory+'02_job_posts_indeed.csv')\n",
    "\n",
    "# Remove all null cleaned_job_title records\n",
    "jobs_descriptions = data[~data.cleaned_job_title.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filter to only jobs specified by the jobs_to_model list\n",
    "jobs_descriptions = jobs_descriptions[jobs_descriptions.cleaned_job_title\\\n",
    "                                      .isin(jobs_to_model.cleaned_job_title)]\n",
    "\n",
    "# Rename job summary field\n",
    "if parameters['doc_type'] == 'indeed_resume':\n",
    "    jobs_descriptions.rename(columns = {'descript':'summary_text'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of resume entries available: 997695/3200066\n"
     ]
    }
   ],
   "source": [
    "# Print the number of records left after removing irrelevant jobs\n",
    "print(\"Number of resume entries available:\", \n",
    "      str(jobs_descriptions.cleaned_job_title.count()) +\"/\"+\n",
    "      str(data.cleaned_job_title.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of jobs with 1000+ resume entries: 152\n",
      "Number of resume entries available now: 849706\n"
     ]
    }
   ],
   "source": [
    "# Remove all jobs without `min_job_summaries` or more resume entries\n",
    "cnt_resumes_available = jobs_descriptions.groupby('cleaned_job_title')\\\n",
    "                                .count().reset_index()\n",
    "cnt_resumes_available = list(cnt_resumes_available[\n",
    "            cnt_resumes_available.summary_text>parameters['min_job_summaries']].cleaned_job_title)\n",
    "jobs_descriptions = jobs_descriptions[jobs_descriptions.cleaned_job_title\\\n",
    "                       .isin(cnt_resumes_available)]\n",
    "\n",
    "# Save off list of resume ids\n",
    "if parameters['doc_type'] == 'indeed_resume':\n",
    "    # Save the list of resume ids for resumes being used\n",
    "    pd.DataFrame(jobs_descriptions.resume_id.unique())\\\n",
    "                .to_csv(directory+'03_relevant_resume_ids.csv',index=False)\n",
    "        \n",
    "# Remove duplicate data\n",
    "jobs_descriptions = jobs_descriptions[['cleaned_job_title','summary_text','from_year']].drop_duplicates()\n",
    "\n",
    "print(\"Number of jobs with \"+str(parameters['min_job_summaries'])+\"+ resume entries:\", len(cnt_resumes_available))\n",
    "print(\"Number of resume entries available now:\", jobs_descriptions.cleaned_job_title.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Salary Data for App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This code saves the cleaned salary information back to the main data folder\n",
    "combined_salaries.to_csv(directory+'03_cleaned_salaries_for_app.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cleaned_job_title    658835\n",
       "summary_text         658835\n",
       "from_year            658835\n",
       "dtype: int64"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code samples the number of records to remove excessive numbers\n",
    "new_job_descriptions = pd.DataFrame()\n",
    "for name, group in jobs_descriptions.groupby('cleaned_job_title'):\n",
    "    if group[group.from_year < 2017].cleaned_job_title.count() > parameters['max_number_records']:\n",
    "        new_job_descriptions = pd.concat([new_job_descriptions,group[group.from_year < 2017]\\\n",
    "            .sort_values(by='from_year', ascending=False).head(parameters['max_number_records'])])\n",
    "    elif group[group.from_year < 2018].cleaned_job_title.count() > parameters['max_number_records']:\n",
    "        new_job_descriptions = pd.concat([new_job_descriptions,group[group.from_year < 2018]\\\n",
    "            .sort_values(by='from_year', ascending=False).head(parameters['max_number_records'])])\n",
    "    else:\n",
    "        new_job_descriptions = pd.concat([new_job_descriptions,group\\\n",
    "            .sort_values(by='from_year', ascending=False).head(parameters['max_number_records'])])\n",
    "\n",
    "new_job_descriptions.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from functions.word_preprocessing\n",
    "x_data = preprocess_list(new_job_descriptions.summary_text)\n",
    "\n",
    "# Create labels using cleaned_job_title\n",
    "y_labels = new_job_descriptions.cleaned_job_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into test and train datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, \n",
    "                                                    y_labels,\n",
    "                                                    test_size=parameters['train_test_split'],\n",
    "                                                    random_state=parameters['random_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: 2018-07-20 11:32:38.994954\n",
      "End: 2018-07-20 11:43:18.055595\n",
      "Vocabulary len: 914318\n"
     ]
    }
   ],
   "source": [
    "print(\"Start:\", datetime.datetime.now())\n",
    "\n",
    "# Train TF-IDF vectorizer model\n",
    "vect = TfidfVectorizer(min_df=parameters['min_df'], \n",
    "                       ngram_range=(parameters['min_ngram'], parameters['max_ngram'])\n",
    "                      ).fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "print(\"End:\", datetime.datetime.now())\n",
    "\n",
    "print('Vocabulary len:', len(vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sm = SMOTE(kind='regular')\n",
    "X_res, y_res = sm.fit_sample(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp_display = pd.DataFrame(y_res)\n",
    "temp_display.columns = ['range']\n",
    "temp_display['counter'] = 1\n",
    "temp_display.groupby('range').count().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 33.18%\n"
     ]
    }
   ],
   "source": [
    "# Train Multinomial Naive Bayes model\n",
    "model = MultinomialNB(alpha=parameters['alpha'])\n",
    "model.fit(X_res, y_res)\n",
    "\n",
    "y_pred = model.predict(vect.transform(X_test))\n",
    "print('Accuracy: %.2f%%' % (accuracy_score(y_test, y_pred) * 100))\n",
    "\n",
    "# Accuracy: 33.18% for skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:  0.314251605066\n",
      "precision_score:  0.337255341797\n",
      "recall_score:  0.323380618768\n"
     ]
    }
   ],
   "source": [
    "print('f1_score: ', f1_score(y_test, y_pred, average=\"macro\"))\n",
    "print('precision_score: ', precision_score(y_test, y_pred, average=\"macro\"))\n",
    "print('recall_score: ', recall_score(y_test, y_pred, average=\"macro\"))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>fscore</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>build and release engineer</td>\n",
       "      <td>0.773973</td>\n",
       "      <td>0.824818</td>\n",
       "      <td>0.798587</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>linux administrator</td>\n",
       "      <td>0.707317</td>\n",
       "      <td>0.773333</td>\n",
       "      <td>0.738854</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>salesforce engineer</td>\n",
       "      <td>0.620915</td>\n",
       "      <td>0.896226</td>\n",
       "      <td>0.733591</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>technical recruiter</td>\n",
       "      <td>0.662577</td>\n",
       "      <td>0.812030</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>ios engineer</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.775510</td>\n",
       "      <td>0.723810</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>android engineer</td>\n",
       "      <td>0.616438</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>database administrator</td>\n",
       "      <td>0.664122</td>\n",
       "      <td>0.713115</td>\n",
       "      <td>0.687747</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>ux engineer</td>\n",
       "      <td>0.643836</td>\n",
       "      <td>0.717557</td>\n",
       "      <td>0.678700</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>devops engineer</td>\n",
       "      <td>0.604167</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.659091</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>network engineer</td>\n",
       "      <td>0.582822</td>\n",
       "      <td>0.748031</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          class  precision    recall    fscore  support\n",
       "20   build and release engineer   0.773973  0.824818  0.798587      137\n",
       "72          linux administrator   0.707317  0.773333  0.738854      150\n",
       "119         salesforce engineer   0.620915  0.896226  0.733591      106\n",
       "144         technical recruiter   0.662577  0.812030  0.729730      133\n",
       "65                 ios engineer   0.678571  0.775510  0.723810       49\n",
       "8              android engineer   0.616438  0.789474  0.692308       57\n",
       "41       database administrator   0.664122  0.713115  0.687747      122\n",
       "151                 ux engineer   0.643836  0.717557  0.678700      131\n",
       "44              devops engineer   0.604167  0.725000  0.659091      120\n",
       "85             network engineer   0.582822  0.748031  0.655172      127"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision, recall, fscore, support = score(y_test, y_pred)\n",
    "\n",
    "metrics = pd.DataFrame(list(zip(model.classes_, precision, recall, fscore, support)))\n",
    "metrics.columns = ['class','precision', 'recall', 'fscore', 'support']\n",
    "metrics.sort_values(by='fscore',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import collections\n",
    "# counted = collections.Counter(y_test)\n",
    "\n",
    "# from operator import itemgetter\n",
    "# import heapq\n",
    "# import collections\n",
    "# def least_common_values(array, to_find=None):\n",
    "#     counter = collections.Counter(array)\n",
    "#     if to_find is None:\n",
    "#         return sorted(counter.items(), key=itemgetter(1), reverse=False)\n",
    "#     return heapq.nsmallest(to_find, counter.items(), key=itemgetter(1))\n",
    "\n",
    "# # counted.most_common()\n",
    "# least_common_values(counted, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predictions = pd.DataFrame(list(zip(y_test, y_pred)))\n",
    "# predictions.columns=['actual','prediction']\n",
    "# predictions['count']=1\n",
    "# predictions.groupby(['actual','prediction']).count().reset_index().to_csv('most_confusion.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List Most Relevant Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "software quality engineer\n",
      "-------\n",
      "black box testing\n",
      "create execute test case\n",
      "hp quality center\n",
      "using selenium webdriver\n",
      "work closely development\n",
      "bug tracking system\n",
      "user acceptance testing\n",
      "automation test script\n",
      "manual automate test\n",
      "development life cycle\n",
      "perform regression testing\n",
      "selenium web driver\n",
      "web base application\n",
      "develop test plan test\n"
     ]
    }
   ],
   "source": [
    "# This code finds the top parameters['num_skills'] of features to show the user. It filters out any \n",
    "# ngram where the same n-1 version of the ngram is shown. This cuts down on repetition.\n",
    "\n",
    "label_id = 125\n",
    "\n",
    "print(model.classes_[label_id])\n",
    "print('-------')\n",
    "\n",
    "features_list = []\n",
    "topn_class1 = sorted(zip(model.coef_[label_id], vect.get_feature_names()))[-parameters['num_skills']:]\n",
    "for coef, feat in topn_class1:\n",
    "    features_list.append(feat)\n",
    "\n",
    "accepted_skill_list = [model.classes_[label_id]]\n",
    "for potential_skill in sorted(features_list, key=lambda x: -len(x.split())):\n",
    "    highest_match = len(potential_skill.split())\n",
    "    for accepted_skill in accepted_skill_list:\n",
    "        leftovers = list(set(potential_skill.split()) - set(accepted_skill.split()))\n",
    "        if len(leftovers) < highest_match:\n",
    "            highest_match = len(leftovers)\n",
    "    if highest_match > 1:\n",
    "        accepted_skill_list.append(potential_skill)\n",
    "accepted_skill_list = accepted_skill_list[1:]\n",
    "shuffle(accepted_skill_list)\n",
    "\n",
    "for skill in accepted_skill_list:\n",
    "    print(skill)\n",
    "    \n",
    "# project coordinator\n",
    "# -------\n",
    "# work breakdown structure\n",
    "# standard operating procedure\n",
    "# project schedule budget\n",
    "# support multiple project\n",
    "# subject matter expert\n",
    "# provide administrative support\n",
    "# monitor project progress\n",
    "# assisting project manager\n",
    "# point contact client\n",
    "# project management process\n",
    "# change order purchase order\n",
    "# minute action item\n",
    "# coordinate project activity\n",
    "# user acceptance testing\n",
    "# project start finish\n",
    "# maintain update project\n",
    "# project life cycle\n",
    "# power point presentation\n",
    "# team sharepoint site\n",
    "# new team member\n",
    "# timely project status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Similarity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2759753    quality assurance tester\n",
      "Name: cleaned_job_title, dtype: object\n",
      "\n",
      "---------------------\n",
      " identify high priority blocking issue build playtest release utilize perforce accomplish multiple branch testing responsibility including workspace setup versioning task developer change list request maintain confluence document studio including best practice testing debugging command feature system extracurricular event attend multiple stand ups interfaced development department use debugging inhouse tool object particle world editor initiate define design test plan core feature detail scheduling assessment risk analysis responsibility dependency track report bug jira along test case creation execution specialize core prototype feature gameplay system ui ux well performance testing telemetry use jenkins monitor track successful completion tool resource across development branch use microsoft visual studio provide callstacks crash dump engineer \n",
      "\n",
      "---------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.30388242325621184, 'software test engineer'),\n",
       " (0.17214933275517827, 'quality assurance analyst'),\n",
       " (0.16843687158465764, 'software quality engineer'),\n",
       " (0.13348509287564367, 'quality assurance tester'),\n",
       " (0.10358515909507994, 'quality assurance engineer'),\n",
       " (0.046884908442811506, 'test engineer'),\n",
       " (0.043943204045823858, 'quality assurance'),\n",
       " (0.0062453140513258612, 'software engineer'),\n",
       " (0.0047259311112173834, 'test'),\n",
       " (0.0023001662415846791, 'software tester'),\n",
       " (0.00171414609841201, 'applications engineer'),\n",
       " (0.0017061199572182037, 'quality assurance manager'),\n",
       " (0.0014999677080099539, 'automation engineer'),\n",
       " (0.001395692550142733, 'engineer'),\n",
       " (0.0012304698425423353, 'programmer analyst'),\n",
       " (0.00097454684245530112, 'technical'),\n",
       " (0.00065948406869555088, 'technical project manager'),\n",
       " (0.00060954120088938543, 'project'),\n",
       " (0.0005130903532594588, 'systems analyst'),\n",
       " (0.00050314200341516186, 'business analyst')]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code returns the prediction probabilities for an example input\n",
    "\n",
    "example_index = 11\n",
    "print(y_test[example_index:example_index+1])\n",
    "\n",
    "print()\n",
    "print(\"---------------------\")\n",
    "print(X_test[example_index])\n",
    "\n",
    "print()\n",
    "print(\"---------------------\")\n",
    "vector_example = vect.transform(preprocess_list([X_test[example_index]]))\n",
    "job_rankings = list(zip(model.predict_proba(vector_example)[0],model.classes_))\n",
    "sorted(job_rankings,reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save New Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20180720111747975469\n"
     ]
    }
   ],
   "source": [
    "# This code saves the model to the models folder\n",
    "directory = '/mnt/disks/mnt_dir/'\n",
    "\n",
    "save_time = re.sub('[^A-Za-z0-9]+', '', str(datetime.datetime.now()))\n",
    "print(save_time)\n",
    "\n",
    "write_param = open(directory+\"models/\" + save_time + '_parameters.txt','w')\n",
    "for key in parameters:\n",
    "    write_param.write(key + \"=\" + str(parameters[key]) + '\\n')\n",
    "write_param.close()\n",
    "\n",
    "# Save preprocessed x data\n",
    "pickling_on = open(directory+\"models/\"+save_time+\"_x_data.pkl\",\"wb\")\n",
    "pickle.dump(x_data, pickling_on)\n",
    "pickling_on.close()\n",
    "\n",
    "# Save preprocessed y labels\n",
    "pickling_on = open(directory+\"models/\"+save_time+\"_y_labels.pkl\",\"wb\")\n",
    "pickle.dump(y_labels, pickling_on)\n",
    "pickling_on.close()\n",
    "\n",
    "# Save preprocessed x SMOTE data\n",
    "pickling_on = open(directory+\"models/\"+save_time+\"_x_SMOTE_data.pkl\",\"wb\")\n",
    "pickle.dump(X_res, pickling_on)\n",
    "pickling_on.close()\n",
    "\n",
    "# Save preprocessed y SMOTE labels\n",
    "pickling_on = open(directory+\"models/\"+save_time+\"_y_SMOTE_labels.pkl\",\"wb\")\n",
    "pickle.dump(y_res, pickling_on)\n",
    "pickling_on.close()\n",
    "\n",
    "# Save TD-IDF vectorizer\n",
    "pickling_on = open(directory+\"models/\"+save_time+\"_tdidf_vect.pkl\",\"wb\")\n",
    "pickle.dump(vect, pickling_on)\n",
    "pickling_on.close()\n",
    "\n",
    "# Save vectorized x_train\n",
    "pickling_on = open(directory+\"models/\"+save_time+\"_x_trained_tdidf_vect.pkl\",\"wb\")\n",
    "pickle.dump(X_train_vectorized, pickling_on)\n",
    "pickling_on.close()\n",
    "\n",
    "# Save NB model\n",
    "pickling_on = open(directory+\"models/\"+save_time+\"_nb_model.pkl\",\"wb\")\n",
    "pickle.dump(model, pickling_on)\n",
    "pickling_on.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code loads an old model\n",
    "directory = '/mnt/disks/mnt_dir/'\n",
    "\n",
    "save_time = '20180720111747975469' # Currently best model for doc similarity\n",
    "\n",
    "pickling_on = open(directory+\"models/\"+save_time+\"_x_data.pkl\",\"rb\")\n",
    "x_data = pickle.load(pickling_on)\n",
    "pickling_on.close()\n",
    "\n",
    "# Save preprocessed y labels\n",
    "pickling_on = open(directory+\"models/\"+save_time+\"_y_labels.pkl\",\"rb\")\n",
    "y_labels = pickle.load(pickling_on)\n",
    "pickling_on.close()\n",
    "\n",
    "# Save TD-IDF vectorizer\n",
    "pickling_on = open(directory+\"models/\"+save_time+\"_tdidf_vect.pkl\",\"rb\")\n",
    "vect = pickle.load(pickling_on)\n",
    "pickling_on.close()\n",
    "\n",
    "# Save vectorized x_train\n",
    "pickling_on = open(directory+\"models/\"+save_time+\"_x_trained_tdidf_vect.pkl\",\"rb\")\n",
    "X_train_vectorized = pickle.load(pickling_on)\n",
    "pickling_on.close()\n",
    "\n",
    "# # Save NB model\n",
    "# pickling_on = open(directory+\"models/\"+save_time+\"_nb_model.pkl\",\"rb\")\n",
    "# model = pickle.load(pickling_on)\n",
    "# pickling_on.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['000 amp',\n",
       " '000 amp nbsp',\n",
       " '000acre',\n",
       " '000acre military',\n",
       " '000acre military installation',\n",
       " '000attendee',\n",
       " '000attendee event',\n",
       " '000attendee event largest',\n",
       " '000ayear',\n",
       " '000ayear digital',\n",
       " '000ayear digital marketing',\n",
       " '000bpd',\n",
       " '000bpd oil',\n",
       " '000bpd oil gas',\n",
       " '000bpsd',\n",
       " '000bpsd barrel',\n",
       " '000bpsd barrel per',\n",
       " '000circulation',\n",
       " '000circulation bimonthly']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vect.get_feature_names()[1:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most likely I will need to train with a 1-5 ngram model and then return skills\n",
    "based on a 3-4 ngram model"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
