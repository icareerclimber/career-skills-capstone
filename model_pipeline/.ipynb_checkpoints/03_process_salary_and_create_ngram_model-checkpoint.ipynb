{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Naive Bayes classifier using TD-IDF vectorizer and pickle results. This model can be \n",
    "# trained on job resumes or Indeed job postings. Steps include:\n",
    "# 1. Set the model parameters\n",
    "# 2. Getting a combined list of salaries and only using job titles with 500+ salary records\n",
    "# 3. Getting a list of resumes using this list of job titles and remove any job titles with \n",
    "#    less than 500 resumes\n",
    "# 4. Run TD-IDF vectorizer and Naive Bayes model training\n",
    "# 5. Test the model using \"List Most Relevant Skills\"\n",
    "# 6. Test the model using \"Document Similarity Score\"\n",
    "\n",
    "\n",
    "# We are only looking at the most recent 5 years of salary\n",
    "# We are only looking at the mode recent 10 years of work start date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from itertools import product\n",
    "from random import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from imblearn.pipeline import Pipeline \n",
    "\n",
    "\n",
    "# Custom function in functions folder\n",
    "from functions.word_preprocessing import *\n",
    "\n",
    "pd.set_option('display.max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '/Users/kwheatley/Desktop/Capstone/gcloud_data/'\n",
    "# directory = '/mnt/disks/mnt_dir/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best parameters for doc similarity\n",
    "parameters = {\n",
    "                \"doc_type\":\"indeed_resume\", # Use \"indeed_resume\" or \"indeed_postings\"\n",
    "                \"min_salary_year\":2014, # Filter out all salaries older than specified number\n",
    "                \"min_job_year\":2008, # Filter out all resume jobs older than specified number\n",
    "                \"min_salary_records\":100, # Filter out all jobs with less than specified salary records\n",
    "                \"min_job_summaries\":1000, # Filter out all jobs with less than specified job summaries\n",
    "                \"min_ngram\":1, # For TD-IDF vectorizer\n",
    "                \"max_ngram\":3, # For TD-IDF vectorizer\n",
    "                \"min_df\":5, # For TD-IDF vectorizer, ignore features in less than this number of documents\n",
    "                \"train_test_split\":0.05, # For train-test split\n",
    "                \"random_state\":1, # For train-test split\n",
    "                \"alpha\":0.02, # For Naive Bayes model 0.02 is current best\n",
    "                \"num_skills\":50, # Number of skill to show per job \n",
    "                \"max_number_records\":2500, # Number of records in each class, SMOTE is used to fill small classes\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Best parameters for skills\n",
    "\n",
    "# parameters = {\n",
    "#                 \"doc_type\":\"indeed_resume\", # Use \"indeed_resume\" or \"indeed_postings\"\n",
    "#                 \"min_salary_year\":2014, # Filter out all salaries older than specified number\n",
    "#                 \"min_job_year\":2008, # Filter out all resume jobs older than specified number\n",
    "#                 \"min_salary_records\":100, # Filter out all jobs with less than specified salary records\n",
    "#                 \"min_job_summaries\":1000, # Filter out all jobs with less than specified job summaries\n",
    "#                 \"min_ngram\":3, # For TD-IDF vectorizer\n",
    "#                 \"max_ngram\":3, # For TD-IDF vectorizer\n",
    "#                 \"min_df\":5, # For TD-IDF vectorizer, ignore features in less than this number of documents\n",
    "#                 \"train_test_split\":0.05, # For train-test split\n",
    "#                 \"random_state\":1, # For train-test split\n",
    "#                 \"alpha\":0.02, # For Naive Bayes model 0.02 is current best\n",
    "#                 \"num_skills\":100, # Number of skill to show per job \n",
    "#                 \"max_number_records\":5000, # Number of records in each class, SMOTE is used to fill small classes\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Salary Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both salary datasets\n",
    "salary1 = pd.read_csv(directory+'02_salaries_h1b.csv')\n",
    "salary2 = pd.read_csv(directory+'02_salaries_greencard.csv')\n",
    "\n",
    "# Combine salary datasets\n",
    "temp_salary1 = salary1[['role','city','state','start_year','cleaned_job_title','experiences','salary']]\n",
    "temp_salary2 = salary2[['job_title','city','state','decision_year','cleaned_job_title','experiences','salary_amount']]\n",
    "temp_salary1.columns = ['original_role','city','state','start_year','cleaned_job_title','experiences','salary']\n",
    "temp_salary2.columns = ['original_role','city','state','start_year','cleaned_job_title','experiences','salary']\n",
    "combined_salaries = temp_salary1.append(temp_salary2)\n",
    "\n",
    "# Remove salaries with null value and convert to int\n",
    "combined_salaries = combined_salaries[~combined_salaries.salary.isnull()]\n",
    "combined_salaries.salary = combined_salaries.salary.astype(int)\n",
    "\n",
    "# Fill any NaN fields with no_value and convert each column into a list\n",
    "combined_salaries = combined_salaries.fillna('no_value')\n",
    "combined_salaries.experiences = combined_salaries.experiences.apply(lambda x: \n",
    "                                                list([item.strip() for item in x.split(',')]))\n",
    "combined_salaries.original_role = combined_salaries.original_role.apply(lambda x: [x])\n",
    "combined_salaries.city = combined_salaries.city.apply(lambda x: [x])\n",
    "combined_salaries.state = combined_salaries.state.apply(lambda x: [x])\n",
    "combined_salaries.start_year = combined_salaries.start_year.apply(lambda x: [x])\n",
    "combined_salaries.cleaned_job_title = combined_salaries.cleaned_job_title.apply(lambda x: [x])\n",
    "combined_salaries.salary = combined_salaries.salary.apply(lambda x: [x])\n",
    "\n",
    "# Perform a pivot on the columns to split out rows with multiple experience level qualifiers\n",
    "combined_salaries = pd.DataFrame([j for i in combined_salaries.values for j in product(*i)],\n",
    "                                      columns = combined_salaries.columns)\n",
    "\n",
    "# Only look at jobs in the past 5 years\n",
    "combined_salaries = combined_salaries[combined_salaries.start_year >= parameters['min_salary_year']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get List of Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of jobs with 100+ salary records: 732\n"
     ]
    }
   ],
   "source": [
    "# Choose all jobs with `min_salary_records` or more records\n",
    "temp = combined_salaries.groupby('cleaned_job_title').count().salary.reset_index()\n",
    "jobs_to_model = temp[temp.salary >= parameters['min_salary_records']]\n",
    "combined_salaries = combined_salaries[combined_salaries.cleaned_job_title\\\n",
    "                                                       .fillna('').isin(jobs_to_model.cleaned_job_title)]\n",
    "print(\"Number of jobs with \"+str(parameters['min_salary_records'])+\"+ salary records:\", jobs_to_model.cleaned_job_title.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Job Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load resume data\n",
    "if parameters['doc_type'] == 'indeed_resume':\n",
    "    data = pd.read_csv(directory+'02_resumes_work.csv')\n",
    "    data.rename(columns = {'descript':'summary_text'}, inplace=True)\n",
    "if parameters['doc_type'] == 'indeed_postings':\n",
    "    data = pd.read_csv(directory+'02_job_posts_indeed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of jobs with 1000+ resume entries: 114\n",
      "Number of resume entries available: 497928\n"
     ]
    }
   ],
   "source": [
    "# Remove all null cleaned_job_title records\n",
    "jobs_descriptions = data[~data.cleaned_job_title.isnull()]\n",
    "\n",
    "# Remove all jobs older than 10 years\n",
    "jobs_descriptions = jobs_descriptions[jobs_descriptions.from_year >= parameters['min_job_year']]\n",
    "\n",
    "# Drop insignificant job names\n",
    "jobs_to_remove = ['technical','team','test','project']\n",
    "jobs_descriptions = jobs_descriptions[~jobs_descriptions.cleaned_job_title.isin(jobs_to_remove)] \n",
    "\n",
    "# Filter to only jobs specified by the jobs_to_model list\n",
    "jobs_descriptions = jobs_descriptions[jobs_descriptions.cleaned_job_title\\\n",
    "                                      .isin(jobs_to_model.cleaned_job_title)]\n",
    "    \n",
    "# Remove all jobs without `min_job_summaries` or more resume entries\n",
    "cnt_resumes_available = jobs_descriptions.groupby('cleaned_job_title')\\\n",
    "                                .count().reset_index()\n",
    "cnt_resumes_available = list(cnt_resumes_available[\n",
    "            cnt_resumes_available.summary_text>parameters['min_job_summaries']].cleaned_job_title)\n",
    "jobs_descriptions = jobs_descriptions[jobs_descriptions.cleaned_job_title\\\n",
    "                       .isin(cnt_resumes_available)]\n",
    "\n",
    "# Remove duplicate data\n",
    "jobs_descriptions = jobs_descriptions.groupby(['cleaned_job_title','summary_text','from_year'])\\\n",
    "        .resume_id.first().reset_index()\n",
    "\n",
    "print(\"Number of jobs with \"+str(parameters['min_job_summaries'])+\"+ resume entries:\", len(cnt_resumes_available))\n",
    "print(\"Number of resume entries available:\", jobs_descriptions.cleaned_job_title.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cleaned_job_title    234714\n",
       "summary_text         234714\n",
       "from_year            234714\n",
       "resume_id            234714\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code samples the number of records to remove excessive numbers\n",
    "new_job_descriptions = pd.DataFrame()\n",
    "for name, group in jobs_descriptions.groupby('cleaned_job_title'):\n",
    "    if group[group.from_year == 2016].cleaned_job_title.count() >= parameters['max_number_records']:\n",
    "        new_job_descriptions = pd.concat([new_job_descriptions,group[group.from_year < 2017]\\\n",
    "            .sort_values(by='from_year', ascending=False).head(parameters['max_number_records'])])\n",
    "    elif group[(group.from_year == 2016) | (group.from_year == 2017)].cleaned_job_title.count() >= parameters['max_number_records']:\n",
    "        new_job_descriptions = pd.concat([new_job_descriptions,group[group.from_year < 2018]\\\n",
    "            .sort_values(by='from_year', ascending=False).head(parameters['max_number_records'])])\n",
    "    else:\n",
    "        new_job_descriptions = pd.concat([new_job_descriptions,group\\\n",
    "            .sort_values(by='from_year', ascending=False).head(parameters['max_number_records'])])\n",
    "\n",
    "new_job_descriptions.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Data for Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save off list of resume ids\n",
    "if parameters['doc_type'] == 'indeed_resume':\n",
    "    # Save the list of resume ids for resumes being used\n",
    "    pd.DataFrame(new_job_descriptions.resume_id.unique())\\\n",
    "                .to_csv(directory+'03_relevant_resume_ids.csv',index=False)\n",
    "\n",
    "# Save off list of relevant job titles\n",
    "relevant_job_titles = pd.DataFrame(new_job_descriptions.cleaned_job_title.unique())\n",
    "relevant_job_titles.columns = ['cleaned_job_title']\n",
    "relevant_job_titles.to_csv(directory+'03_relevant_job_titles.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code saves the cleaned salary information back to the main data folder\n",
    "combined_salaries.to_csv(directory+'03_cleaned_salaries_for_app.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = new_job_descriptions.summary_text\n",
    "y_labels = new_job_descriptions.cleaned_job_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into test and train datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, \n",
    "                                                    y_labels,\n",
    "                                                    test_size=parameters['train_test_split'],\n",
    "                                                    random_state=parameters['random_state'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: 2018-07-28 08:13:46.112849\n",
      "Done with preprocess\n",
      "Done with TD-IDF\n",
      "Vocabulary len: 1031373\n",
      "End: 2018-07-28 08:31:21.942723\n"
     ]
    }
   ],
   "source": [
    "print(\"Start:\", datetime.datetime.now())\n",
    "\n",
    "x_data = new_job_descriptions.summary_text\n",
    "y_labels = new_job_descriptions.cleaned_job_title\n",
    "\n",
    "x_data = preprocess_list(new_job_descriptions.summary_text)\n",
    "print(\"Done with preprocess\")\n",
    "\n",
    "# Split the data into test and train datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, \n",
    "                                                    y_labels,\n",
    "                                                    test_size=parameters['train_test_split'],\n",
    "                                                    random_state=parameters['random_state'])\n",
    "\n",
    "# Train TF-IDF vectorizer model\n",
    "vect = TfidfVectorizer(min_df=parameters['min_df'], \n",
    "                       ngram_range=(parameters['min_ngram'], parameters['max_ngram'])\n",
    "                      ).fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "print(\"Done with TD-IDF\")\n",
    "\n",
    "print('Vocabulary len:', len(vect.get_feature_names()))\n",
    "\n",
    "sm = SMOTE(kind='regular')\n",
    "X_res, y_res = sm.fit_sample(X_train_vectorized, y_train)\n",
    "\n",
    "print(\"End:\", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickling_on = open(directory+\"models/dev_X_test.pkl\",\"wb\")\n",
    "pickle.dump(X_test, pickling_on)\n",
    "pickling_on.close()\n",
    "\n",
    "pickling_on = open(directory+\"models/dev_y_test.pkl\",\"wb\")\n",
    "pickle.dump(y_test, pickling_on)\n",
    "pickling_on.close()\n",
    "\n",
    "pickling_on = open(directory+\"models/dev_x_SMOTE_data.pkl\",\"wb\")\n",
    "pickle.dump(X_res, pickling_on)\n",
    "pickling_on.close()\n",
    "\n",
    "pickling_on = open(directory+\"models/vect.pkl\",\"wb\")\n",
    "pickle.dump(vect, pickling_on)\n",
    "pickling_on.close()\n",
    "\n",
    "\n",
    "# pickling_on = open(directory+\"models/dev_X_test.pkl\",\"rb\")\n",
    "# X_test = pickle.load(pickling_on)\n",
    "# pickling_on.close()\n",
    "\n",
    "# pickling_on = open(directory+\"models/dev_X_test.pkl\",\"rb\")\n",
    "# X_test = pickle.load(pickling_on)\n",
    "# pickling_on.close()\n",
    "\n",
    "# pickling_on = open(directory+\"models/dev_y_test.pkl\",\"rb\")\n",
    "# y_test = pickle.load(pickling_on)\n",
    "# pickling_on.close()\n",
    "\n",
    "# pickling_on = open(directory+\"models/dev_x_SMOTE_data.pkl\",\"rb\")\n",
    "# X_res = pickle.load(pickling_on)\n",
    "# pickling_on.close()\n",
    "\n",
    "# pickling_on = open(directory+\"models/dev_y_SMOTE_labels.pkl\",\"rb\")\n",
    "# y_res = pickle.load(pickling_on)\n",
    "# pickling_on.close()\n",
    "\n",
    "# pickling_on = open(directory+\"models/vect.pkl\",\"wb\")\n",
    "# pickle.dump(vect, pickling_on)\n",
    "# pickling_on.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 48.90%\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB(alpha=0.02)\n",
    "model.fit(X_res, y_res)\n",
    "\n",
    "y_pred = model.predict(vect.transform(X_test))\n",
    "print('Accuracy: %.2f%%' % (accuracy_score(y_test, y_pred) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: 2018-07-28 10:28:49.561347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kwheatley/anaconda/envs/python36/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:73: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 46.85%\n",
      "End: 2018-07-28 10:34:02.868062\n"
     ]
    }
   ],
   "source": [
    "print(\"Start:\", datetime.datetime.now())\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "svm_model = SGDClassifier(loss='hinge', penalty='l2', alpha=.0012, n_iter=5, random_state=42)\n",
    "\n",
    "svm_model.fit(X_res, y_res)\n",
    "\n",
    "y_pred = svm_model.predict(vect.transform(X_test))\n",
    "print('Accuracy: %.2f%%' % (accuracy_score(y_test, y_pred) * 100))\n",
    "\n",
    "print(\"End:\", datetime.datetime.now())\n",
    "# Accuracy: 47.06% .001\n",
    "# Accuracy: 47.03% .0008\n",
    "# Accuracy: 46.97% .01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: 2018-07-28 09:15:22.899457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kwheatley/anaconda/envs/python36/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:73: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.12%\n",
      "End: 2018-07-28 09:27:33.729109\n"
     ]
    }
   ],
   "source": [
    "print(\"Start:\", datetime.datetime.now())\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "svm_model = SGDClassifier(loss='hinge', penalty='l1', alpha=.001, n_iter=5, random_state=42)\n",
    "\n",
    "svm_model.fit(X_res, y_res)\n",
    "\n",
    "y_pred = svm_model.predict(vect.transform(X_test))\n",
    "print('Accuracy: %.2f%%' % (accuracy_score(y_test, y_pred) * 100))\n",
    "\n",
    "print(\"End:\", datetime.datetime.now())\n",
    "# Accuracy: 18.10% .0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start:\", datetime.datetime.now())\n",
    "\n",
    "def create_model_architecture(input_size):\n",
    "    # create input layer \n",
    "    input_layer = layers.Input((input_size, ), sparse=True)\n",
    "    \n",
    "    # create hidden layer\n",
    "    hidden_layer = layers.Dense(100, activation=\"relu\")(input_layer)\n",
    "    \n",
    "    # create output layer\n",
    "    output_layer = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
    "\n",
    "    classifier = models.Model(inputs = input_layer, outputs = output_layer)\n",
    "    classifier.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    return classifier \n",
    "\n",
    "classifier = create_model_architecture(xtrain_tfidf_ngram.shape[1])\n",
    "\n",
    "classifier.fit(X_res, y_res)\n",
    "\n",
    "y_pred = classifier.predict(vect.transform(X_test))\n",
    "print('Accuracy: %.2f%%' % (accuracy_score(y_test, y_pred) * 100))\n",
    "\n",
    "print(\"End:\", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_cnn()\n",
    "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "print \"CNN, Word Embeddings\",  accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vectorized_X_train = count_vect.fit_transform(X_train)\n",
    "\n",
    "sm = SMOTE(kind='regular')\n",
    "count_vect_X_res, count_vect_y_res = sm.fit_sample(count_vectorized_X_train, y_train)\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "# xtrain_count =  count_vect.transform(train_x)\n",
    "count_vectorized_X_test =  count_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF on Count Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\n",
    "print \"RF, Count Vectors: \", accuracy\n",
    "\n",
    "# RF on Word Level TF IDF Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print \"RF, WordLevel TF-IDF: \", accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "\n",
    "# clf1 = LogisticRegression(random_state=1)\n",
    "# clf2 = RandomForestClassifier(random_state=1)\n",
    "# clf3 = MultinomialNB(alpha=parameters['alpha'])\n",
    "\n",
    "# eclf1 = VotingClassifier(estimators=[\n",
    "#             ('lr', clf1), ('rf', clf2), ('nb', clf3)], voting='hard')\n",
    "# eclf2 = VotingClassifier(estimators=[\n",
    "#             ('lr', clf1), ('rf', clf2), ('nb', clf3)], voting='soft')\n",
    "# eclf3 = VotingClassifier(estimators=[\n",
    "#             ('lr', clf1), ('rf', clf2), ('nb', clf3)], voting='soft', weights=[1,1,2], flatten_transform=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: 2018-07-26 11:32:46.196836\n"
     ]
    }
   ],
   "source": [
    "print(\"Start:\", datetime.datetime.now())\n",
    "clf2.fit(X_res, y_res)\n",
    "y_pred = clf1.predict(vect.transform(X_test))\n",
    "print('Accuracy: %.2f%%' % (accuracy_score(y_test, y_pred) * 100))\n",
    "print(\"End:\", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start:\", datetime.datetime.now())\n",
    "clf2.fit(X_res, y_res)\n",
    "y_pred = clf2.predict(vect.transform(X_test))\n",
    "print('Accuracy: %.2f%%' % (accuracy_score(y_test, y_pred) * 100))\n",
    "print(\"End:\", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start:\", datetime.datetime.now())\n",
    "clf3.fit(X_res, y_res)\n",
    "y_pred = clf3.predict(vect.transform(X_test))\n",
    "print('Accuracy: %.2f%%' % (accuracy_score(y_test, y_pred) * 100))\n",
    "print(\"End:\", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start:\", datetime.datetime.now())\n",
    "eclf1.fit(X_res, y_res)\n",
    "y_pred = eclf1.predict(vect.transform(X_test))\n",
    "print('Accuracy: %.2f%%' % (accuracy_score(y_test, y_pred) * 100))\n",
    "print(\"End:\", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start:\", datetime.datetime.now())\n",
    "eclf2.fit(X_res, y_res)\n",
    "y_pred = eclf2.predict(vect.transform(X_test))\n",
    "print('Accuracy: %.2f%%' % (accuracy_score(y_test, y_pred) * 100))\n",
    "print(\"End:\", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start:\", datetime.datetime.now())\n",
    "eclf3.fit(X_res, y_res)\n",
    "y_pred = eclf3.predict(vect.transform(X_test))\n",
    "print('Accuracy: %.2f%%' % (accuracy_score(y_test, y_pred) * 100))\n",
    "print(\"End:\", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca = PCA(n_components=152).fit(X_res)\n",
    "data2D = pca.transform(X_res)\n",
    "plt.scatter(data2D[:,0], data2D[:,1], c=data.target)\n",
    "plt.show()              #not required if using ipython notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    "            xticklabels=category_id_df.Product.values, yticklabels=category_id_df.Product.values)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocess = FunctionTransformer(preprocess_list)\n",
    "smt = SMOTE()\n",
    "tfidf = TfidfVectorizer(min_df=parameters['min_df'], ngram_range=(parameters['min_ngram'], parameters['max_ngram']))\n",
    "nb = MultinomialNB(alpha=parameters['alpha'])\n",
    "preprocess = FunctionTransformer(preprocess_list, validate=False)\n",
    "\n",
    "pipeline = Pipeline([('preprocess', preprocess), ('tfidf', tfidf), ('smt', smt), ('nb', nb)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: 459707\n",
      "y_train: 459707\n",
      "Start: 2018-07-24 23:43:16.778521\n",
      "End: 2018-07-25 00:07:21.414853\n"
     ]
    }
   ],
   "source": [
    "print('X_train:', len(X_train))\n",
    "print('y_train:', len(y_train))\n",
    "\n",
    "print(\"Start:\", datetime.datetime.now())\n",
    "pipeline.fit(X_train,y_train)\n",
    "print(\"End:\", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 35.53%\n"
     ]
    }
   ],
   "source": [
    "y_pred = pipeline.predict(X_test)\n",
    "print('Accuracy: %.2f%%' % (accuracy_score(y_test, y_pred) * 100))\n",
    "\n",
    "# Accuracy: 44.81% for doc similarity\n",
    "# Accuracy: 35.53% for skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:  0.323935852893\n",
      "precision_score:  0.336582884615\n",
      "recall_score:  0.332000061005\n"
     ]
    }
   ],
   "source": [
    "print('f1_score: ', f1_score(y_test, y_pred, average=\"macro\"))\n",
    "print('precision_score: ', precision_score(y_test, y_pred, average=\"macro\"))\n",
    "print('recall_score: ', recall_score(y_test, y_pred, average=\"macro\"))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>fscore</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>specialist</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.030769</td>\n",
       "      <td>0.043011</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>operations analyst</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.040541</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>ceo</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.044118</td>\n",
       "      <td>0.058252</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>business development</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.073171</td>\n",
       "      <td>0.070175</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>assistant</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.054422</td>\n",
       "      <td>0.084656</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    class  precision    recall    fscore  support\n",
       "129            specialist   0.071429  0.030769  0.043011       65\n",
       "87     operations analyst   0.078947  0.040541  0.053571       74\n",
       "31                    ceo   0.085714  0.044118  0.058252       68\n",
       "23   business development   0.067416  0.073171  0.070175       82\n",
       "12              assistant   0.190476  0.054422  0.084656      147"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision, recall, fscore, support = score(y_test, y_pred)\n",
    "\n",
    "metrics = pd.DataFrame(list(zip(pipeline.classes_, precision, recall, fscore, support)))\n",
    "metrics.columns = ['class','precision', 'recall', 'fscore', 'support']\n",
    "metrics.sort_values(by='fscore',ascending=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import collections\n",
    "# counted = collections.Counter(y_test)\n",
    "\n",
    "# from operator import itemgetter\n",
    "# import heapq\n",
    "# import collections\n",
    "# def least_common_values(array, to_find=None):\n",
    "#     counter = collections.Counter(array)\n",
    "#     if to_find is None:\n",
    "#         return sorted(counter.items(), key=itemgetter(1), reverse=False)\n",
    "#     return heapq.nsmallest(to_find, counter.items(), key=itemgetter(1))\n",
    "\n",
    "# # counted.most_common()\n",
    "# least_common_values(counted, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>prediction</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>accountant</td>\n",
       "      <td>staff accountant</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2369</th>\n",
       "      <td>human resources specialist</td>\n",
       "      <td>human resources manager</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4811</th>\n",
       "      <td>staff accountant</td>\n",
       "      <td>accountant</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3225</th>\n",
       "      <td>process engineer</td>\n",
       "      <td>manufacturing engineer</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4946</th>\n",
       "      <td>systems administrator</td>\n",
       "      <td>network administrator</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2620</th>\n",
       "      <td>j2ee engineer</td>\n",
       "      <td>java software engineer</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2636</th>\n",
       "      <td>java software engineer</td>\n",
       "      <td>j2ee engineer</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3032</th>\n",
       "      <td>network administrator</td>\n",
       "      <td>systems administrator</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1304</th>\n",
       "      <td>controller</td>\n",
       "      <td>accounting manager</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5096</th>\n",
       "      <td>teacher</td>\n",
       "      <td>preschool teacher</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5074</th>\n",
       "      <td>systems engineer</td>\n",
       "      <td>systems administrator</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2811</th>\n",
       "      <td>marketing director</td>\n",
       "      <td>marketing manager</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3967</th>\n",
       "      <td>quality assurance engineer</td>\n",
       "      <td>quality assurance analyst</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>assistant manager</td>\n",
       "      <td>store manager</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>accounting manager</td>\n",
       "      <td>controller</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2923</th>\n",
       "      <td>mechanical engineer</td>\n",
       "      <td>mechanical design engineer</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2526</th>\n",
       "      <td>it manager</td>\n",
       "      <td>network administrator</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3932</th>\n",
       "      <td>quality assurance analyst</td>\n",
       "      <td>quality assurance tester</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3568</th>\n",
       "      <td>programmer analyst</td>\n",
       "      <td>net engineer</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3024</th>\n",
       "      <td>network administrator</td>\n",
       "      <td>network engineer</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          actual                  prediction  count\n",
       "117                   accountant            staff accountant     61\n",
       "2369  human resources specialist     human resources manager     61\n",
       "4811            staff accountant                  accountant     53\n",
       "3225            process engineer      manufacturing engineer     51\n",
       "4946       systems administrator       network administrator     48\n",
       "2620               j2ee engineer      java software engineer     47\n",
       "2636      java software engineer               j2ee engineer     42\n",
       "3032       network administrator       systems administrator     42\n",
       "1304                  controller          accounting manager     40\n",
       "5096                     teacher           preschool teacher     38\n",
       "5074            systems engineer       systems administrator     38\n",
       "2811          marketing director           marketing manager     37\n",
       "3967  quality assurance engineer   quality assurance analyst     37\n",
       "609            assistant manager               store manager     36\n",
       "164           accounting manager                  controller     36\n",
       "2923         mechanical engineer  mechanical design engineer     36\n",
       "2526                  it manager       network administrator     34\n",
       "3932   quality assurance analyst    quality assurance tester     33\n",
       "3568          programmer analyst                net engineer     33\n",
       "3024       network administrator            network engineer     33"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From the test population, get the most confused labels\n",
    "\n",
    "# pd.options.display.max_rows = 10000\n",
    "predictions = pd.DataFrame(list(zip(y_test, y_pred)))\n",
    "predictions.columns=['actual','prediction']\n",
    "predictions['count']=1\n",
    "pred_group = predictions.groupby(['actual','prediction']).count().reset_index()\n",
    "pred_group[(pred_group.actual != pred_group.prediction) \n",
    "           & (pred_group.prediction!='account executive')\n",
    "          ].sort_values(by='count',ascending=False).head(20)\n",
    "# .to_csv('most_confusion.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List Most Relevant Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product manager\n",
      "-------\n",
      "define product requirement\n",
      "io android apps\n",
      "market research competitive\n",
      "within first month\n",
      "life cycle management\n",
      "work closely engineering\n",
      "successful product launch\n",
      "responsible product management\n",
      "key performance indicator\n",
      "quality assurance team\n",
      "product development process\n",
      "product strategy vision\n",
      "work crossfunctional team\n",
      "agile scrum team\n",
      "create product roadmap\n",
      "product road map\n",
      "people get job\n",
      "conduct competitive analysis\n",
      "manage entire product\n",
      "increase market share\n",
      "agile software development\n",
      "lead product team\n",
      "minimum viable product\n",
      "business case development\n",
      "team engineer designer\n",
      "internal external customer\n",
      "support sale team\n",
      "prioritize product backlog\n",
      "user experience design\n",
      "story acceptance criteria\n",
      "content management system\n",
      "lead cross functional\n",
      "writing user story\n",
      "new product service\n",
      "using agile methodology\n",
      "user acceptance testing\n",
      "improve customer experience\n",
      "serve product owner\n",
      "go market strategy\n",
      "new business opportunity\n",
      "subject matter expert\n"
     ]
    }
   ],
   "source": [
    "# This code finds the top parameters['num_skills'] of features to show the user. It filters out any \n",
    "# ngram where the same n-1 version of the ngram is shown. This cuts down on repetition.\n",
    "\n",
    "label_id = 93\n",
    "\n",
    "print(pipeline.classes_[label_id])\n",
    "print('-------')\n",
    "\n",
    "features_list = []\n",
    "topn_class1 = sorted(zip(pipeline.named_steps['nb'].coef_[label_id], \n",
    "                         pipeline.named_steps['tfidf'].get_feature_names()))[-parameters['num_skills']:]\n",
    "for coef, feat in topn_class1:\n",
    "    features_list.append(feat)\n",
    "\n",
    "accepted_skill_list = [pipeline.classes_[label_id]]\n",
    "for potential_skill in sorted(features_list, key=lambda x: -len(x.split())):\n",
    "    highest_match = len(potential_skill.split())\n",
    "    for accepted_skill in accepted_skill_list:\n",
    "        leftovers = list(set(potential_skill.split()) - set(accepted_skill.split()))\n",
    "        if len(leftovers) < highest_match:\n",
    "            highest_match = len(leftovers)\n",
    "    if highest_match > 1:\n",
    "        accepted_skill_list.append(potential_skill)\n",
    "accepted_skill_list = accepted_skill_list[1:]\n",
    "shuffle(accepted_skill_list)\n",
    "\n",
    "for skill in accepted_skill_list:\n",
    "    print(skill)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Similarity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299648    consultant\n",
      "Name: cleaned_job_title, dtype: object\n",
      "\n",
      "---------------------\n",
      "299648    .as part of academically sponsored team-based project .• Conducted industry research and competitive benchmarking in order to develop pricing strategy .• Determined feasibility of program based on financial analysis of Bay Cove Academy.\n",
      "Name: summary_text, dtype: object\n",
      "\n",
      "---------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.22726698136641141, 'financial analyst'),\n",
       " (0.10846793866030462, 'marketing specialist'),\n",
       " (0.090911912318914545, 'chief financial officer'),\n",
       " (0.076218383979244672, 'business development manager'),\n",
       " (0.047572097864376342, 'analyst'),\n",
       " (0.037165532420553309, 'technical writer'),\n",
       " (0.033379704417007695, 'consultant'),\n",
       " (0.024299218120848249, 'business manager'),\n",
       " (0.021285235634691892, 'product marketing manager'),\n",
       " (0.018763913424894845, 'program manager'),\n",
       " (0.014960565077392386, 'accounting'),\n",
       " (0.013389055843904769, 'marketing manager'),\n",
       " (0.013364387274254676, 'product manager'),\n",
       " (0.009982822959913918, 'business development'),\n",
       " (0.0098005846809103812, 'business consultant'),\n",
       " (0.0083345510721833452, 'sales manager'),\n",
       " (0.0077160757680939246, 'operations manager'),\n",
       " (0.007528294906168775, 'buyer'),\n",
       " (0.0072209841150210619, 'operations analyst'),\n",
       " (0.0067130493753592216, 'creative director')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code returns the prediction probabilities for an example input\n",
    "\n",
    "example_index = 40\n",
    "print(y_test[example_index:example_index+1])\n",
    "\n",
    "print()\n",
    "print(\"---------------------\")\n",
    "example = X_test[example_index:example_index+1]\n",
    "print(example)\n",
    "\n",
    "print()\n",
    "print(\"---------------------\")\n",
    "\n",
    "job_rankings = list(zip(pipeline.predict_proba(example)[0],pipeline.classes_))\n",
    "sorted(job_rankings,reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save New Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20180725002822539809\n"
     ]
    }
   ],
   "source": [
    "# This code saves the model to the models folder\n",
    "directory = '/mnt/disks/mnt_dir/'\n",
    "\n",
    "save_time = re.sub('[^A-Za-z0-9]+', '', str(datetime.datetime.now()))\n",
    "print(save_time)\n",
    "\n",
    "write_param = open(directory+\"models/\" + save_time + '_parameters.txt','w')\n",
    "for key in parameters:\n",
    "    write_param.write(key + \"=\" + str(parameters[key]) + '\\n')\n",
    "write_param.close()\n",
    "\n",
    "# Save preprocessed x data\n",
    "pickling_on = open(directory+\"models/\"+save_time+\"_x_data.pkl\",\"wb\")\n",
    "pickle.dump(x_data, pickling_on)\n",
    "pickling_on.close()\n",
    "\n",
    "# Save preprocessed y labels\n",
    "pickling_on = open(directory+\"models/\"+save_time+\"_y_labels.pkl\",\"wb\")\n",
    "pickle.dump(y_labels, pickling_on)\n",
    "pickling_on.close()\n",
    "\n",
    "# Save NB model\n",
    "pickling_on = open(directory+\"models/\"+save_time+\"_pipeline_model.pkl\",\"wb\")\n",
    "pickle.dump(pipeline, pickling_on)\n",
    "pickling_on.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This code loads an old model\n",
    "directory = '/mnt/disks/mnt_dir/'\n",
    "\n",
    "# save_time = '20180724220628349336' # Currently best model for doc similarity\n",
    "save_time = '20180725002822539809' # Currently best model for skills\n",
    "\n",
    "pickling_on = open(directory+\"models/\"+save_time+\"_x_data.pkl\",\"rb\")\n",
    "x_data = pickle.load(pickling_on)\n",
    "pickling_on.close()\n",
    "\n",
    "pickling_on = open(directory+\"models/\"+save_time+\"_y_labels.pkl\",\"rb\")\n",
    "y_labels = pickle.load(pickling_on)\n",
    "pickling_on.close()\n",
    "\n",
    "pickling_on = open(directory+\"models/\"+save_time+\"_pipeline_model.pkl\",\"rb\")\n",
    "pipeline = pickle.load(pickling_on)\n",
    "pickling_on.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python36]",
   "language": "python",
   "name": "conda-env-python36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
