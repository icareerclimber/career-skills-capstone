{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('processed_resumes_work_ADDED_JOB_TITLES.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove jobs without title\n",
    "data = data[~data.role.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual transformations of similar jobs\n",
    "manual_update_data = pd.read_csv('manual_job_title_transformations.csv', encoding='latin-1')\n",
    "manual_update_job_dict = manual_update_data[~manual_update_data.manual_change.isnull()]\\\n",
    "    [['cleaned_job_title','manual_change']]\\\n",
    "    .set_index('cleaned_job_title')['manual_change']\\\n",
    "    .to_dict()\n",
    "    \n",
    "    \n",
    "# Words that will be manually changed\n",
    "manual_change_dict = {'developers':'developer',\n",
    "                 'develppers':'developer',\n",
    "                 'coordinators':'coordinator',\n",
    "                 'application':'applications',\n",
    "                 'analysts':'analyst',\n",
    "                 'analyast':'analyst',\n",
    "                 'ananlyst':'analyst',\n",
    "                 'analysr':'analyst',\n",
    "                 'information technology':'it',\n",
    "                 'managers':'manager',\n",
    "                 'srprogrammer':'sr programmer',\n",
    "                 'srcomputer':'sr computer',\n",
    "                 'srjava':'sr java',\n",
    "                 'srsoftware':'sr software',\n",
    "                 'srqa':'sr qa',\n",
    "                 'srapplication':'sr application',\n",
    "                 'srsystem':'sr systems',\n",
    "                 'salesforcecom':'salesforce',\n",
    "                 'admin':'administrator',\n",
    "                 'developer':'engineer',\n",
    "                 'solution':'solutions',\n",
    "                 'net':'.net',\n",
    "                 'dotnet':'.net',\n",
    "                 'fullstack':'full stack',\n",
    "                 'db':'database',\n",
    "                 'system':'systems',\n",
    "                 'principle':'principal',\n",
    "                 'ui':'ux',\n",
    "                 'user experience':'ux',\n",
    "                 'ui/ux':'ux',\n",
    "                 'ux/ui':'ux',\n",
    "                 'wi-fi':'wifi',\n",
    "                 'wi fi':'wifi',\n",
    "                 'user interface':'ux',\n",
    "                 'dba':'database administrator',\n",
    "                 'enigneer':'engineer',\n",
    "                 'enginerr':'engineer',\n",
    "                 'engingeer':'engineer',\n",
    "                 'engneer':'engineer',\n",
    "                 'engr':'engineer',\n",
    "                 'hr':'human resources',\n",
    "                 'manger':'manager',\n",
    "                 'qa':'quality assurance',\n",
    "                 'frontend':'front end',\n",
    "                 'backend':'back end',\n",
    "                 'bi':'business intelligence',\n",
    "                 'vp':'vice president',\n",
    "                 'sr':'senior',\n",
    "                 'jr':'junior',\n",
    "                 'i':'1',\n",
    "                 'ii':'2',\n",
    "                 'iii':'3',\n",
    "                 'iv':'4',\n",
    "                 'v':'5',\n",
    "                 'vi':'6',\n",
    "                 'vii':'7',\n",
    "                 'l1':'1',\n",
    "                 'l2':'2',\n",
    "                 'l3':'3',\n",
    "                 'l4':'4',\n",
    "                 'l5':'5',\n",
    "                 'l6':'6',\n",
    "                 'l7':'7',\n",
    "                 'one':'1',\n",
    "                 'two':'2',\n",
    "                 'three':'3',\n",
    "                 'four':'4',\n",
    "                 'five':'5',\n",
    "                 'six':'6',\n",
    "                 'seven':'7',\n",
    "                 'a':'1',\n",
    "                 'b':'2',\n",
    "                 'c':'3',\n",
    "                 ' d':'4',\n",
    "                 '-d':'4',\n",
    "                 'e':'5',\n",
    "                 'f':'6',\n",
    "                 'g':'7',\n",
    "                }\n",
    "\n",
    "# Words that will be removed from job title\n",
    "trash_words_list = ['jc', 'mts', 'level']\n",
    "\n",
    "# Job prefix qualifiers that will be moved to experience list\n",
    "pre_qualifiers = ['lead']\n",
    "\n",
    "# Job postscript qualifiers that will be moved to experience list\n",
    "post_qualifiers = []\n",
    "\n",
    "# Job qualifiers that will be moved to experience list\n",
    "any_location_qualifiers = [\n",
    "                    'vice president',\n",
    "                    'president',\n",
    "                    'principal', \n",
    "                    'senior', \n",
    "                    'junior',\n",
    "                    'entry', \n",
    "                    'mid', \n",
    "                    'intern',\n",
    "                    '1', '2', '3', '4', '5', '6', '7',\n",
    "                ]\n",
    "\n",
    "\n",
    "\n",
    "# This function moves qualifiers from job title list to experience list\n",
    "def parse_experience(list_of_jobs):\n",
    "    clean_job_list = []\n",
    "    experience_list = []\n",
    "    for job in list_of_jobs:\n",
    "        single_job_experience = []\n",
    "        for word in pre_qualifiers:\n",
    "            regex = re.compile('^'+word+'\\\\b', re.I)\n",
    "            if re.search(regex, job):\n",
    "                single_job_experience.append(word)\n",
    "                job = re.sub(regex, '', job)\n",
    "#         for word in post_qualifiers:\n",
    "#             regex = re.compile('\\s*\\\\b'+word+'(\\W*?)$', re.I)\n",
    "#             result = re.search(regex, job)\n",
    "#             if re.search(regex, job):\n",
    "#                 single_job_experience.append(word)\n",
    "#                 job = re.sub(regex, '', job)\n",
    "        for word in any_location_qualifiers:\n",
    "            regex = re.compile('\\\\b'+word+'\\\\b', re.I)\n",
    "            result = re.search(regex, job)\n",
    "            if re.search(regex, job):\n",
    "                single_job_experience.append(word)\n",
    "                job = re.sub(regex, '', job)\n",
    "        clean_job_list.append(job)\n",
    "        experience_list.append(single_job_experience)\n",
    "    return clean_job_list, experience_list\n",
    "\n",
    "def trash_words(list_of_jobs):\n",
    "    cleaned_list_of_jobs = []\n",
    "    for job in list_of_jobs:\n",
    "        for word in trash_words_list:\n",
    "            regex = re.compile('\\\\b'+word+'[0-9]*(\\s|\\\\b)', re.I)\n",
    "            job = re.sub(regex, '', job)\n",
    "        cleaned_list_of_jobs.append(job)\n",
    "    return cleaned_list_of_jobs\n",
    "        \n",
    "def remove_special_characters(char_list, list_of_jobs):\n",
    "    for char in char_list:\n",
    "        list_of_jobs = [job.replace(char,'') for job in list_of_jobs]\n",
    "    return list_of_jobs\n",
    "\n",
    "def manual_update_words(list_of_jobs):\n",
    "    cleaned_list_of_jobs = []\n",
    "    for job in list_of_jobs:\n",
    "        for key in manual_change_dict:\n",
    "            regex = re.compile(\"\\\\b\"+key+\"\\\\b\")\n",
    "            job = re.sub(regex, manual_change_dict[key], job)\n",
    "        cleaned_list_of_jobs.append(job)\n",
    "    return cleaned_list_of_jobs\n",
    "\n",
    "def manual_update_job_titles(list_of_jobs):\n",
    "    cleaned_list_of_jobs = []\n",
    "    for job in list_of_jobs:\n",
    "        if job in manual_update_job_dict:\n",
    "            job = manual_update_job_dict[job]\n",
    "        cleaned_list_of_jobs.append(job)\n",
    "    return cleaned_list_of_jobs\n",
    "\n",
    "def remove_words_in_parenthesis(list_of_jobs):\n",
    "    cleaned_list_of_jobs = []\n",
    "    for job in list_of_jobs:\n",
    "        job = re.sub(re.compile(\"\\((.*?)\\)\"), '', job)\n",
    "        job = re.sub(re.compile(\"\\[(.*?)\\]\"), '', job)\n",
    "        cleaned_list_of_jobs.append(job)\n",
    "    return cleaned_list_of_jobs\n",
    "\n",
    "def remove_words_after_special_char(char_list, list_of_jobs):\n",
    "    for char in char_list:\n",
    "        list_of_jobs = [job.split(char, -1)[0] for job in list_of_jobs]\n",
    "    return list_of_jobs\n",
    "\n",
    "def clean_job(list_of_jobs):\n",
    "    \n",
    "    # Lowercase and strip whitespaces\n",
    "    clean_job_list = [job.strip().lower() for job in list_of_jobs]\n",
    "\n",
    "    # Remove special characters . and : and ;\n",
    "    clean_job_list = remove_special_characters( ['.',':',';','#'] , clean_job_list)\n",
    "    \n",
    "    # Remove specific words\n",
    "    clean_job_list = trash_words(clean_job_list)\n",
    "\n",
    "    # Manually update words using list\n",
    "    clean_job_list = manual_update_words(clean_job_list)\n",
    "\n",
    "    # First round pull out qualifiers\n",
    "    clean_job_list, experience_list1 = parse_experience(clean_job_list)\n",
    "    \n",
    "    # Delete all content between parenthesis (...) or [...]\n",
    "    clean_job_list = remove_words_in_parenthesis(clean_job_list)\n",
    "\n",
    "    # Remove all text after - and / and (\n",
    "    clean_job_list = remove_words_after_special_char([' -', '- ', ' - ','/','(',')'], clean_job_list)\n",
    "\n",
    "    # Replace - with ' '\n",
    "    clean_job_list = [job.replace('-',' ') for job in clean_job_list]\n",
    "\n",
    "    # For strings with comma, reverse the order and remove comma\n",
    "    clean_job_list = [job.split(',', 1)[1].strip() + ' ' + job.split(',', 1)[0].strip() \n",
    "                      if len(job.split(',', 1))>1 else job\n",
    "                      for job in clean_job_list]\n",
    "    \n",
    "    # If there is more than 1 comma, remove the text for the 2nd\n",
    "    clean_job_list = remove_words_after_special_char([','], clean_job_list)\n",
    "    \n",
    "    # Manually update job titles using list\n",
    "    clean_job_list = manual_update_job_titles(clean_job_list)\n",
    "\n",
    "    # Second round pull out qualifiers\n",
    "    clean_job_list, experience_list2 = parse_experience(clean_job_list)\n",
    "    \n",
    "    # Clean up extra whitespaces\n",
    "    clean_job_list = [job.replace('  ',' ').strip() for job in clean_job_list]\n",
    "    \n",
    "    # Remove any numbers\n",
    "    clean_job_list = [x for x in clean_job_list if not isinstance(x, int)]\n",
    "\n",
    "    # Merge the 2 rounds of qualifier grabbing\n",
    "    experience_list = list(map(list.__add__, experience_list1, experience_list2))        \n",
    "\n",
    "    return list_of_jobs, clean_job_list, experience_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_of_jobs, clean_job_list, experience_list = clean_job(data.role)\n",
    "clean_job_list = manual_update_job_titles(clean_job_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_experience_list = []\n",
    "for item in experience_list:\n",
    "    sorted_experience_list.append(sorted(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['converted_job_title_new'] = clean_job_list\n",
    "# data['converted_experience_level_new'] = sorted_experience_list\n",
    "# data['converted_experience_level_new'] = data['converted_experience_level_new'].astype(str)\n",
    "# data['converted_experience_level_new'] = data.converted_experience_level_new.apply(lambda x: x.replace('[','').replace(']','').replace(\"'\",''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>city</th>\n",
       "      <th>resume_id</th>\n",
       "      <th>container</th>\n",
       "      <th>role</th>\n",
       "      <th>company</th>\n",
       "      <th>location</th>\n",
       "      <th>dates</th>\n",
       "      <th>descript</th>\n",
       "      <th>converted_job_title</th>\n",
       "      <th>converted_experience_level</th>\n",
       "      <th>converted_job_title_new</th>\n",
       "      <th>converted_experience_level_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>atlanta</td>\n",
       "      <td>0004d469fc497102</td>\n",
       "      <td>work-experience-items</td>\n",
       "      <td>senior informix database administrator</td>\n",
       "      <td>Breckinridge Insurance</td>\n",
       "      <td>Kennesaw, GA</td>\n",
       "      <td>July 2017 to Present</td>\n",
       "      <td>.Informix DBA for Breckinridge Insurance appli...</td>\n",
       "      <td>administrator database</td>\n",
       "      <td>['senior']</td>\n",
       "      <td>informix database administrator</td>\n",
       "      <td>senior</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>atlanta</td>\n",
       "      <td>0004d469fc497102</td>\n",
       "      <td>work-experience-items</td>\n",
       "      <td>senior informix dba database administrator</td>\n",
       "      <td>INTERCALL Inc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>January 2007 to June 2017</td>\n",
       "      <td>.Informix Database Administor for InterCall's ...</td>\n",
       "      <td>administrator database dba</td>\n",
       "      <td>['senior']</td>\n",
       "      <td>informix database administrator database admin...</td>\n",
       "      <td>senior</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>atlanta</td>\n",
       "      <td>0004d469fc497102</td>\n",
       "      <td>work-experience-items</td>\n",
       "      <td>oracle informix dba database administrator</td>\n",
       "      <td>Accenture/Bellsouth Telecommuncations Inc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>March 2004 to December 2007</td>\n",
       "      <td>.Oracle Database for OPEDS production support....</td>\n",
       "      <td>administrator database oracle</td>\n",
       "      <td>[]</td>\n",
       "      <td>oracle informix database administrator databas...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>atlanta</td>\n",
       "      <td>0004d469fc497102</td>\n",
       "      <td>work-experience-items</td>\n",
       "      <td>peoplesoft hrms oracle dba database administrator</td>\n",
       "      <td>ACENTRON/Michelin Inc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>March 2003 to February 2004</td>\n",
       "      <td>.Responsible for Migrating objects and Project...</td>\n",
       "      <td>administrator database oracle</td>\n",
       "      <td>[]</td>\n",
       "      <td>peoplesoft hrms oracle database administrator ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>atlanta</td>\n",
       "      <td>0004d469fc497102</td>\n",
       "      <td>work-experience-items</td>\n",
       "      <td>informix oracle database administrator</td>\n",
       "      <td>BellSouth Telecommunications INC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>November 1998 to January 2003</td>\n",
       "      <td>.Worked on various projects for BellSouth. Wor...</td>\n",
       "      <td>administrator database oracle</td>\n",
       "      <td>[]</td>\n",
       "      <td>informix oracle database administrator</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     city         resume_id              container  \\\n",
       "0           0  atlanta  0004d469fc497102  work-experience-items   \n",
       "1           1  atlanta  0004d469fc497102  work-experience-items   \n",
       "2           2  atlanta  0004d469fc497102  work-experience-items   \n",
       "3           3  atlanta  0004d469fc497102  work-experience-items   \n",
       "4           4  atlanta  0004d469fc497102  work-experience-items   \n",
       "\n",
       "                                                role  \\\n",
       "0             senior informix database administrator   \n",
       "1         senior informix dba database administrator   \n",
       "2         oracle informix dba database administrator   \n",
       "3  peoplesoft hrms oracle dba database administrator   \n",
       "4             informix oracle database administrator   \n",
       "\n",
       "                                     company      location  \\\n",
       "0                     Breckinridge Insurance  Kennesaw, GA   \n",
       "1                              INTERCALL Inc           NaN   \n",
       "2  Accenture/Bellsouth Telecommuncations Inc           NaN   \n",
       "3                      ACENTRON/Michelin Inc           NaN   \n",
       "4           BellSouth Telecommunications INC           NaN   \n",
       "\n",
       "                           dates  \\\n",
       "0           July 2017 to Present   \n",
       "1      January 2007 to June 2017   \n",
       "2    March 2004 to December 2007   \n",
       "3    March 2003 to February 2004   \n",
       "4  November 1998 to January 2003   \n",
       "\n",
       "                                            descript  \\\n",
       "0  .Informix DBA for Breckinridge Insurance appli...   \n",
       "1  .Informix Database Administor for InterCall's ...   \n",
       "2  .Oracle Database for OPEDS production support....   \n",
       "3  .Responsible for Migrating objects and Project...   \n",
       "4  .Worked on various projects for BellSouth. Wor...   \n",
       "\n",
       "             converted_job_title converted_experience_level  \\\n",
       "0         administrator database                 ['senior']   \n",
       "1     administrator database dba                 ['senior']   \n",
       "2  administrator database oracle                         []   \n",
       "3  administrator database oracle                         []   \n",
       "4  administrator database oracle                         []   \n",
       "\n",
       "                             converted_job_title_new  \\\n",
       "0                    informix database administrator   \n",
       "1  informix database administrator database admin...   \n",
       "2  oracle informix database administrator databas...   \n",
       "3  peoplesoft hrms oracle database administrator ...   \n",
       "4             informix oracle database administrator   \n",
       "\n",
       "  converted_experience_level_new  \n",
       "0                         senior  \n",
       "1                         senior  \n",
       "2                                 \n",
       "3                                 \n",
       "4                                 "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(['converted_job_title_new']).descript.count().to_csv('resume_titles_count.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_jobs = pd.read_csv('data_pivot.csv', encoding='latin-1')\n",
    "# relevant_jobs['total'] = relevant_jobs['2010']+\\\n",
    "#                          relevant_jobs['2011']+\\\n",
    "#                          relevant_jobs['2012']+\\\n",
    "#                          relevant_jobs['2013']+\\\n",
    "#                          relevant_jobs['2014']+\\\n",
    "#                          relevant_jobs['2015']+\\\n",
    "#                          relevant_jobs['2016']+\\\n",
    "#                          relevant_jobs['2017']+\\\n",
    "#                          relevant_jobs['2018']\n",
    "relevant_jobs = relevant_jobs.sort_values(by='total',ascending=False).head(200).cleaned_job_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = data[data.converted_job_title_new.isin(relevant_jobs)]\n",
    "filtered_data = filtered_data[~filtered_data.descript.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list_jobs = filtered_data.groupby('converted_job_title_new').resume_id.count()\\\n",
    ".sort_values(ascending=False).head(50).reset_index().converted_job_title_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_list_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = filtered_data[filtered_data.converted_job_title_new.isin(final_list_jobs)].converted_job_title_new\n",
    "text = filtered_data[filtered_data.converted_job_title_new.isin(final_list_jobs)].descript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_contractions(text):\n",
    "    \"\"\"Replace contractions in string of text\"\"\"\n",
    "    return contractions.fix(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for x in text:\n",
    "#     print(x)\n",
    "#     print()\n",
    "    x = replace_contractions(x)\n",
    "    x = nltk.word_tokenize(x)\n",
    "    tokens.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "#     words = replace_numbers(words)\n",
    "    words = remove_stopwords(words)\n",
    "    words = lemmatize_verbs(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_tokens = []\n",
    "for x in tokens:\n",
    "#     print(x)\n",
    "#     print()\n",
    "    norm_tokens.append(normalize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "traindocs = pickle.load(open('norm_tokens', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189748"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(traindocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickling_on = open(\"norm_tokens\",\"wb\")\n",
    "pickle.dump(norm_tokens, pickling_on)\n",
    "pickling_on.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_text = []\n",
    "for i in norm_tokens:\n",
    "    norm_text.append(' '.join(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickling_on = open(\"norm_text\",\"wb\")\n",
    "pickle.dump(norm_text, pickling_on)\n",
    "pickling_on.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(norm_text, \n",
    "                                                    labels,\n",
    "                                                    test_size=0.10,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary len: 219169\n",
      "Longest word: wwwcomscorecominsightspressreleases20176comscoreandctrannounceexpandedmultiplatformandmobilemeasurementinchina\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# vect = CountVectorizer().fit(X_train)\n",
    "# print('Vocabulary len:', len(vect.get_feature_names()))\n",
    "# print('Longest word:', max(vect.vocabulary_, key=len))\n",
    "\n",
    "# X_train_vectorized = vect.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# model = MultinomialNB(alpha=0.1)\n",
    "# model.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get the feature names as numpy array\n",
    "# feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "# # Sort the coefficients from the model\n",
    "# sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "# # Find the 10 smallest and 10 largest coefficients\n",
    "# # The 10 largest coefficients are being indexed using [:-11:-1] \n",
    "# # so the list returned is in order of largest to smallest\n",
    "# print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "# print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# y_pred = model.predict(vect.transform(X_test))\n",
    "# print('Accuracy: %.2f%%' % (accuracy_score(y_test, y_pred) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary len: 807363\n",
      "Longest word: modalpopupextender maskededitextender maskededitvalidator\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vect = TfidfVectorizer(min_df=5, ngram_range=(1, 3)).fit(X_train)\n",
    "print('Vocabulary len:', len(vect.get_feature_names()))\n",
    "print('Longest word:', max(vect.vocabulary_, key=len))\n",
    "\n",
    "X_train_vectorized = vect.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultinomialNB(alpha=0.1)\n",
    "model.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['00' 'plan db' 'plan debug' 'plan debug software' 'plan decision'\n",
      " 'plan decision make' 'plan decisionmaking' 'plan decisions'\n",
      " 'plan decommission' 'plan daytoday']\n",
      "\n",
      "Largest Coefs: \n",
      "['use' 'aspnet' 'web' 'net' 'sql' 'server' 'application' 'sql server'\n",
      " 'data' 'develop']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 46.97%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = model.predict(vect.transform(X_test))\n",
    "print('Accuracy: %.2f%%' % (accuracy_score(y_test, y_pred) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tokens = pd.DataFrame(list(zip(norm_tokens, labels)))\n",
    "list_tokens.columns = ['tokens','job']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_data.groupby('converted_job_title_new').resume_id.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tokens = list_tokens[list_tokens.job.isin(relevant_jobs)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_tokens.groupby('job').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list_tokens.job\n",
    "text = list_tokens.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_text = []\n",
    "for i in text:\n",
    "    norm_text.append(' '.join(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(norm_text, \n",
    "                                                    labels,\n",
    "                                                    test_size=0.10,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vect = TfidfVectorizer(min_df=5, ngram_range=(4, 8)).fit(X_train)\n",
    "print('Vocabulary len:', len(vect.get_feature_names()))\n",
    "print('Longest word:', max(vect.vocabulary_, key=len))\n",
    "\n",
    "X_train_vectorized = vect.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB(alpha=0.1)\n",
    "model.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = model.predict(vect.transform(X_test))\n",
    "print('Accuracy: %.2f%%' % (accuracy_score(y_test, y_pred) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(list(zip(y_test, y_pred)))\n",
    "predictions.columns=['actual','prediction']\n",
    "predictions['count']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.groupby(['actual','prediction']).count().reset_index().to_csv('confusion.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1136602"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.coef_[40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_id = 20\n",
    "n= 15\n",
    "\n",
    "print(model.classes_[label_id])\n",
    "print('-------')\n",
    "\n",
    "topn_class1 = sorted(zip(model.coef_[label_id], feature_names))[-n:]\n",
    "for coef, feat in topn_class1:\n",
    "    print (feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_proba('worked in sql server and wrote reports')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_proba"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python36]",
   "language": "python",
   "name": "conda-env-python36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
